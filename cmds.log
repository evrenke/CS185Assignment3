Command Log for Assignment 3

QUESTION 1:
For each of the 100+100 files you produced in assignment #2; 
normalize the helpfulness by dividing the helpful_votes by total_votes; 
then find the median of normalized helpfulness scores in each file (google how to use awk for this).

//For this question, the "customerIDs.txt" is filled with 100 top most review giving customers IDs, 
//and CUSTOMERS/id.txt is respectively filled with all of a customers reviews based on their id
//Normalizing reviews is done by checking total votes,
//if its zero the review is given a 0 helpfulness value,
//otherwise its the ratio of the helpful votes to total
//Then, median value is taken from all of the normalized helpfulness scores
//Output is the median value and customer ID to a file
 1545  input="customerIDs.txt"; while read -r line; do awk -F"\t" ' { if($10 !=0) a[i++]=$9 / $10; else a[i++]=0; } END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2, "\t", '$line'; else print a[x-1], "\t", '$line'; }' "CUSTOMERS/$line.txt" >> customerMedians.txt; done < "$input"

QUESTION 2:
Then convert the helpfulness column in each file to binary 0 or 1 values:
a helpfulness score becomes 0 if it is below the median,
and 1 if it is over the median (for that particular file).

//Answer is connected to question 3 answer

QUESTION 3:
Each result should be placed in a new file (don't modify the old files).
For example the results for CUSTOMERS/customerid.txt will be in CUSTOMERS/customerid.BINARY.txt;
the results for PRODUCTS/productid.txt will be in PRODUCTS/productid.BINARY.txt

//For this, the median and ID's from the "customerMedians.txt" file is the critical info used.
//This allows for a second iteration of the reviews, 
//and this time the normalized helpfulness and rating scores are 
// output to a BINARY.txt file for each customer ID.

 1546  input="customerMedians.txt"; while read -r median id; do awk -F"\t" '{ if ($10 != 0 && ($9 / $10) >= '$median') print 1," ", $8; else print 0, " ", $8; }' "CUSTOMERS/$id.txt" > "CUSTOMERS/$id.BINARY.txt"; done < "$input"
 
 QUESTION 4:
 Recompute the correlation scores between rating stars and helpfulness,
 as you did in a2 but using the binarized helpfulness values.
 
 //For this, datamash is used to calculate corrolation values for each customers normalized helpfulness and rating scores
 //This is a new list of a corrolation number associated with a customer ID
 //Output to a new file "customerCorrolations.txt"
 
 
 1547  for FILE in CUSTOMERS/*.BINARY.txt; do datamash-1.3/datamash  -W ppearson 1:2 <  "$FILE" | sed "s|$| $(echo $FILE | tr -cd '[[:digit:]]')|g"; done > "customerCorrolations.txt"
 
 QUESTION 5:
 Plot the review rating stars and helpfulness scores for the top customer
 (the customer with the highest correlation score) in a line plot.
 You can find the gnuplot commands here: http://www.gnuplotting.org/plotting-data/
 
 //For this question, gnuplot did not cooperate on the IBM server, due to lack of access to root.
 //The data to make the plot is created below, but
 //The actual plot was made on google sheets, and is linked in the readme file
//Using a sorted version of the corrolation scores file, and the head command,
 //we can get the highest corrolated customer,
 //the one that fits the best for low helpfulness to low rating
 
 1548  sort -g -r -k 1 "customerCorrolations.txt" | head -n 1
 1549  sort -k 1 "CUSTOMERS/51065232.BINARY.txt"> "CUSTOMERS/highest.BINARYSORTED.txt"
 1550  awk '{print NR, $2}' "CUSTOMERS/highest.BINARYSORTED.txt" > "CUSTOMERS/highest.BINARYSORTEDRATINGS.txt"
 1551  awk '{print NR, $1}' "CUSTOMERS/highest.BINARYSORTED.txt" > "CUSTOMERS/highest.BINARYSORTEDHELPFULNESS.txt"
 
 //QUESTION 6: 
 Plot the review rating stars and helpfulness scores for the top product
 (the product with the highest correlation score) in another line plot.
 
 //This question is a repetition of questions 1-5, but this time for the top corrolation product instead of customer.
 //The steps below parallel the customer questions from above
 
 //Q1 repeated:
 1552  input="productIDs.txt"; while read -r line; do awk -F'\t' -v "ID=$line" '{ if($10 != 0) a[i++]=$9 / $10; else a[i++]=0; } END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2, "\t", ID; else print a[x-1], "\t", ID; }' "PRODUCTS/$line.txt" >> productMedians.txt; done < "$input"
 
 //Q2 and Q3 repeated:
 1553  input="productMedians.txt"; while read -r median id; do awk -F"\t" '{ if ($10 != 0 && ($9 / $10) >= '$median') print 1," ", $8; else print 0, " ", $8; }' "PRODUCTS/$id.txt" > "PRODUCTS/$id.BINARY.txt"; done < "$input"
 
 //Q4 repeated:
 1554  for FILE in PRODUCTS/*.BINARY.txt; do datamash-1.3/datamash  -W ppearson 1:2 <  "$FILE" | sed "s|$| $(echo $FILE | tr -cd '[[:digit:]]')|g"; done > "productCorrolations.txt"
 
 //Q5 repeated:
 1555  sort -g -r -k 1 "productCorrolations.txt" | head -n 1
 1556  sort -k 1 "PRODUCTS/0525947647.BINARY.txt"> "PRODUCTS/highest.BINARYSORTED.txt"
 1557  awk '{print NR, $2}' "PRODUCTS/highest.BINARYSORTED.txt" > "PRODUCTS/highest.BINARYSORTEDRATINGS.txt"
 1558  awk '{print NR, $1}' "PRODUCTS/highest.BINARYSORTED.txt" > "PRODUCTS/highest.BINARYSORTEDHELPFULNESS.txt"
 
 //QUESTION 7:
 Do you think the correlation scores are more meaningful now than before in a2?
 Explain in a few sentences what you think.
 
 //The corrolation scores obtained show the value of a review with more accuracy.
 //Without a normalized helpfulness score, the corrolation was not meaningful to each review, only to other reviews
 //But making it a percentage of total votes on the review shows how much users found help from each review
 //Getting the corrolation from normalized helpfulness to ratings also show that people find reviews more helpful if its more positive
 //Something we couldn't conclude from a non-normalized helpfulness corrolation.
 
 //QUESTION 8:
 In a previous worksheet #7 you extracted words (tokens), html tags, and removed a few stop words for one product reviews file.
 You will do a similar thing. First put in a file the review_body and helpfulness scores for one product
 (you want to use a product file that has plenty of reviews, such that you have enough data to work with;
 you can also use the same product as for worksheet #7 if you prefer).
 
 //For this question I chose the most reviewed product, which has the id 043935806X
 //Putting the helpfulness scores and review body into a file is just a question of awk and cutting, with a pipe to connect them:
 
 1559  awk -v "ID=043935806X" '$4 == ID' amazon_reviews_us_Books_v1_02.tsv | cut -f 9,14 > "043935806X.REVIEWSANDHELPFULNESS.txt"
 
 //QUESTION 9:
 Remove the stop words and html tags from the review_body.
 Then also remove all small 1-2 character words
 (any words with one character or two characters) from the review body.
 
 //Answer of question 9 connected to 10
 
 QUESTION 10:
 Which 10 words appear most frequently in the helpfulness=1 category?
 Which 10 words appear most frequently in the helpfulness=0 category? 
 
 //To get the top 10 words from either category, we need to first isolate the category, using the helpfulness column
 //Then, we can get rid of it to only look at the full review.
 //To get the top 10 words, we can do the cuts from question 9;
 //starting with commas, dots, semicolons, and the key words "and" "or" "if" "it" "in"
 //We also cut all one or two character words here using
 //Then, we  cut away all html tags
 //Finally cut away all spaces into new line changes,
 //And sort each line and count unique words from them
 //This gives us the counts of all words, and we can use head and look at the top 10 words
 
 1560  awk '$1 == 1' "043935806X.REVIEWSANDHELPFULNESS.txt" | cut -f 2 | sed 's/,//g;s/\.//g;s/\;//g;s/\<and\>//g;s/\<or\>//g;s/\<if\>//g;s/\<in\>//g;s/\<it\>//g;s/\<.\>//g;s/\<..\>//g' | sed 's/<[^>]*>//g' | sed -E -e 's/[[:blank:]]+/\n/g' | sort -r | uniq -c | sort -nrk 1 | head -n 10
 1561  awk '$1 == 0' "043935806X.REVIEWSANDHELPFULNESS.txt" | cut -f 2 | sed 's/,//g;s/\.//g;s/\;//g;s/\<and\>//g;s/\<or\>//g;s/\<if\>//g;s/\<in\>//g;s/\<it\>//g;s/\<.\>//g;s/\<..\>//g' | sed 's/<[^>]*>//g' | sed -E -e 's/[[:blank:]]+/\n/g' | sort -r | uniq -c | sort -nrk 1 | head -n 10
 1562  history > cmds.log
